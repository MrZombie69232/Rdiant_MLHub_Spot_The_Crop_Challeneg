{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# Importing libraries to plot graphs\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data_train1.csv')\n",
    "df2 = pd.read_csv('data_train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.114147</td>\n",
       "      <td>30.607718</td>\n",
       "      <td>58.736336</td>\n",
       "      <td>21.596462</td>\n",
       "      <td>29.223473</td>\n",
       "      <td>57.065918</td>\n",
       "      <td>20.252410</td>\n",
       "      <td>27.516077</td>\n",
       "      <td>50.303860</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.237942</td>\n",
       "      <td>48.397106</td>\n",
       "      <td>15.840836</td>\n",
       "      <td>15.077170</td>\n",
       "      <td>51.408360</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.043480</td>\n",
       "      <td>46.913044</td>\n",
       "      <td>73.913040</td>\n",
       "      <td>34.391304</td>\n",
       "      <td>46.695652</td>\n",
       "      <td>73.347824</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>36.173912</td>\n",
       "      <td>56.478260</td>\n",
       "      <td>26.739130</td>\n",
       "      <td>37.695652</td>\n",
       "      <td>55.391304</td>\n",
       "      <td>20.782608</td>\n",
       "      <td>27.565218</td>\n",
       "      <td>49.347828</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.167095</td>\n",
       "      <td>63.654243</td>\n",
       "      <td>94.773780</td>\n",
       "      <td>23.071980</td>\n",
       "      <td>35.622110</td>\n",
       "      <td>62.772495</td>\n",
       "      <td>27.208227</td>\n",
       "      <td>39.453728</td>\n",
       "      <td>65.179950</td>\n",
       "      <td>26.467867</td>\n",
       "      <td>39.048843</td>\n",
       "      <td>63.336760</td>\n",
       "      <td>30.428020</td>\n",
       "      <td>41.363754</td>\n",
       "      <td>60.556557</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>45.847683</td>\n",
       "      <td>61.059600</td>\n",
       "      <td>82.986755</td>\n",
       "      <td>40.158940</td>\n",
       "      <td>54.165560</td>\n",
       "      <td>76.940400</td>\n",
       "      <td>28.092714</td>\n",
       "      <td>39.218544</td>\n",
       "      <td>58.317883</td>\n",
       "      <td>35.629140</td>\n",
       "      <td>47.847683</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>45.384106</td>\n",
       "      <td>59.947020</td>\n",
       "      <td>81.132454</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>29.994318</td>\n",
       "      <td>37.738636</td>\n",
       "      <td>80.744316</td>\n",
       "      <td>29.147728</td>\n",
       "      <td>36.630680</td>\n",
       "      <td>73.278410</td>\n",
       "      <td>27.312500</td>\n",
       "      <td>35.318180</td>\n",
       "      <td>71.267044</td>\n",
       "      <td>24.971590</td>\n",
       "      <td>31.988636</td>\n",
       "      <td>77.835230</td>\n",
       "      <td>21.352272</td>\n",
       "      <td>25.613636</td>\n",
       "      <td>64.704544</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   field_id          0          1          2          3          4          5  \\\n",
       "0       1.0  23.114147  30.607718  58.736336  21.596462  29.223473  57.065918   \n",
       "1       2.0  32.043480  46.913044  73.913040  34.391304  46.695652  73.347824   \n",
       "2       3.0  45.167095  63.654243  94.773780  23.071980  35.622110  62.772495   \n",
       "3       4.0  45.847683  61.059600  82.986755  40.158940  54.165560  76.940400   \n",
       "4       6.0  29.994318  37.738636  80.744316  29.147728  36.630680  73.278410   \n",
       "\n",
       "           6          7          8          9         10         11  \\\n",
       "0  20.252410  27.516077  50.303860  23.000000  30.237942  48.397106   \n",
       "1  24.000000  36.173912  56.478260  26.739130  37.695652  55.391304   \n",
       "2  27.208227  39.453728  65.179950  26.467867  39.048843  63.336760   \n",
       "3  28.092714  39.218544  58.317883  35.629140  47.847683  67.000000   \n",
       "4  27.312500  35.318180  71.267044  24.971590  31.988636  77.835230   \n",
       "\n",
       "          12         13         14  label  \n",
       "0  15.840836  15.077170  51.408360    4.0  \n",
       "1  20.782608  27.565218  49.347828    7.0  \n",
       "2  30.428020  41.363754  60.556557    6.0  \n",
       "3  45.384106  59.947020  81.132454    8.0  \n",
       "4  21.352272  25.613636  64.704544    4.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove three columns as index base\n",
    "df1.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82881, 16)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.114147</td>\n",
       "      <td>30.607718</td>\n",
       "      <td>58.736336</td>\n",
       "      <td>21.596462</td>\n",
       "      <td>29.223473</td>\n",
       "      <td>57.065918</td>\n",
       "      <td>20.252410</td>\n",
       "      <td>27.516077</td>\n",
       "      <td>50.303860</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.237942</td>\n",
       "      <td>48.397106</td>\n",
       "      <td>15.840836</td>\n",
       "      <td>15.077170</td>\n",
       "      <td>51.408360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.043480</td>\n",
       "      <td>46.913044</td>\n",
       "      <td>73.913040</td>\n",
       "      <td>34.391304</td>\n",
       "      <td>46.695652</td>\n",
       "      <td>73.347824</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>36.173912</td>\n",
       "      <td>56.478260</td>\n",
       "      <td>26.739130</td>\n",
       "      <td>37.695652</td>\n",
       "      <td>55.391304</td>\n",
       "      <td>20.782608</td>\n",
       "      <td>27.565218</td>\n",
       "      <td>49.347828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.167095</td>\n",
       "      <td>63.654243</td>\n",
       "      <td>94.773780</td>\n",
       "      <td>23.071980</td>\n",
       "      <td>35.622110</td>\n",
       "      <td>62.772495</td>\n",
       "      <td>27.208227</td>\n",
       "      <td>39.453728</td>\n",
       "      <td>65.179950</td>\n",
       "      <td>26.467867</td>\n",
       "      <td>39.048843</td>\n",
       "      <td>63.336760</td>\n",
       "      <td>30.428020</td>\n",
       "      <td>41.363754</td>\n",
       "      <td>60.556557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>45.847683</td>\n",
       "      <td>61.059600</td>\n",
       "      <td>82.986755</td>\n",
       "      <td>40.158940</td>\n",
       "      <td>54.165560</td>\n",
       "      <td>76.940400</td>\n",
       "      <td>28.092714</td>\n",
       "      <td>39.218544</td>\n",
       "      <td>58.317883</td>\n",
       "      <td>35.629140</td>\n",
       "      <td>47.847683</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>45.384106</td>\n",
       "      <td>59.947020</td>\n",
       "      <td>81.132454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>29.994318</td>\n",
       "      <td>37.738636</td>\n",
       "      <td>80.744316</td>\n",
       "      <td>29.147728</td>\n",
       "      <td>36.630680</td>\n",
       "      <td>73.278410</td>\n",
       "      <td>27.312500</td>\n",
       "      <td>35.318180</td>\n",
       "      <td>71.267044</td>\n",
       "      <td>24.971590</td>\n",
       "      <td>31.988636</td>\n",
       "      <td>77.835230</td>\n",
       "      <td>21.352272</td>\n",
       "      <td>25.613636</td>\n",
       "      <td>64.704544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   field_id          0          1          2          3          4          5  \\\n",
       "0       1.0  23.114147  30.607718  58.736336  21.596462  29.223473  57.065918   \n",
       "1       2.0  32.043480  46.913044  73.913040  34.391304  46.695652  73.347824   \n",
       "2       3.0  45.167095  63.654243  94.773780  23.071980  35.622110  62.772495   \n",
       "3       4.0  45.847683  61.059600  82.986755  40.158940  54.165560  76.940400   \n",
       "4       6.0  29.994318  37.738636  80.744316  29.147728  36.630680  73.278410   \n",
       "\n",
       "           6          7          8          9         10         11  \\\n",
       "0  20.252410  27.516077  50.303860  23.000000  30.237942  48.397106   \n",
       "1  24.000000  36.173912  56.478260  26.739130  37.695652  55.391304   \n",
       "2  27.208227  39.453728  65.179950  26.467867  39.048843  63.336760   \n",
       "3  28.092714  39.218544  58.317883  35.629140  47.847683  67.000000   \n",
       "4  27.312500  35.318180  71.267044  24.971590  31.988636  77.835230   \n",
       "\n",
       "          12         13         14  \n",
       "0  15.840836  15.077170  51.408360  \n",
       "1  20.782608  27.565218  49.347828  \n",
       "2  30.428020  41.363754  60.556557  \n",
       "3  45.384106  59.947020  81.132454  \n",
       "4  21.352272  25.613636  64.704544  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14.844051</td>\n",
       "      <td>49.840836</td>\n",
       "      <td>62.491962</td>\n",
       "      <td>73.43569</td>\n",
       "      <td>48.863342</td>\n",
       "      <td>13.385852</td>\n",
       "      <td>46.858520</td>\n",
       "      <td>59.025723</td>\n",
       "      <td>73.668810</td>\n",
       "      <td>...</td>\n",
       "      <td>41.763664</td>\n",
       "      <td>51.731510</td>\n",
       "      <td>79.36656</td>\n",
       "      <td>58.991962</td>\n",
       "      <td>9.366559</td>\n",
       "      <td>42.319935</td>\n",
       "      <td>50.348873</td>\n",
       "      <td>50.265274</td>\n",
       "      <td>32.673634</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20.521740</td>\n",
       "      <td>62.478260</td>\n",
       "      <td>77.782610</td>\n",
       "      <td>94.82609</td>\n",
       "      <td>66.478264</td>\n",
       "      <td>24.304348</td>\n",
       "      <td>60.217392</td>\n",
       "      <td>74.826090</td>\n",
       "      <td>96.869570</td>\n",
       "      <td>...</td>\n",
       "      <td>48.521740</td>\n",
       "      <td>57.913044</td>\n",
       "      <td>88.04348</td>\n",
       "      <td>73.173910</td>\n",
       "      <td>13.086957</td>\n",
       "      <td>43.173912</td>\n",
       "      <td>50.652172</td>\n",
       "      <td>63.913044</td>\n",
       "      <td>47.347828</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>29.984575</td>\n",
       "      <td>80.282776</td>\n",
       "      <td>99.856040</td>\n",
       "      <td>134.85219</td>\n",
       "      <td>103.096400</td>\n",
       "      <td>13.336761</td>\n",
       "      <td>49.039845</td>\n",
       "      <td>69.282776</td>\n",
       "      <td>111.029564</td>\n",
       "      <td>...</td>\n",
       "      <td>50.354755</td>\n",
       "      <td>67.370180</td>\n",
       "      <td>107.10411</td>\n",
       "      <td>78.559130</td>\n",
       "      <td>20.420310</td>\n",
       "      <td>51.289204</td>\n",
       "      <td>64.726220</td>\n",
       "      <td>104.325195</td>\n",
       "      <td>83.660670</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>33.019867</td>\n",
       "      <td>72.152320</td>\n",
       "      <td>87.238410</td>\n",
       "      <td>128.99338</td>\n",
       "      <td>111.370860</td>\n",
       "      <td>28.245033</td>\n",
       "      <td>64.211920</td>\n",
       "      <td>78.933780</td>\n",
       "      <td>118.099335</td>\n",
       "      <td>...</td>\n",
       "      <td>57.397350</td>\n",
       "      <td>70.059600</td>\n",
       "      <td>108.34437</td>\n",
       "      <td>91.423840</td>\n",
       "      <td>31.549670</td>\n",
       "      <td>70.642390</td>\n",
       "      <td>83.642390</td>\n",
       "      <td>126.701990</td>\n",
       "      <td>110.629140</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>19.170454</td>\n",
       "      <td>66.335230</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>91.10227</td>\n",
       "      <td>60.363636</td>\n",
       "      <td>18.426136</td>\n",
       "      <td>61.147728</td>\n",
       "      <td>76.948860</td>\n",
       "      <td>90.153410</td>\n",
       "      <td>...</td>\n",
       "      <td>63.232956</td>\n",
       "      <td>81.590910</td>\n",
       "      <td>86.17614</td>\n",
       "      <td>54.676136</td>\n",
       "      <td>12.329545</td>\n",
       "      <td>52.948864</td>\n",
       "      <td>67.443184</td>\n",
       "      <td>73.170456</td>\n",
       "      <td>46.937500</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   field_id          0          1          2          3           4  \\\n",
       "0       1.0  14.844051  49.840836  62.491962   73.43569   48.863342   \n",
       "1       2.0  20.521740  62.478260  77.782610   94.82609   66.478264   \n",
       "2       3.0  29.984575  80.282776  99.856040  134.85219  103.096400   \n",
       "3       4.0  33.019867  72.152320  87.238410  128.99338  111.370860   \n",
       "4       6.0  19.170454  66.335230  84.000000   91.10227   60.363636   \n",
       "\n",
       "           5          6          7           8  ...         16         17  \\\n",
       "0  13.385852  46.858520  59.025723   73.668810  ...  41.763664  51.731510   \n",
       "1  24.304348  60.217392  74.826090   96.869570  ...  48.521740  57.913044   \n",
       "2  13.336761  49.039845  69.282776  111.029564  ...  50.354755  67.370180   \n",
       "3  28.245033  64.211920  78.933780  118.099335  ...  57.397350  70.059600   \n",
       "4  18.426136  61.147728  76.948860   90.153410  ...  63.232956  81.590910   \n",
       "\n",
       "          18         19         20         21         22          23  \\\n",
       "0   79.36656  58.991962   9.366559  42.319935  50.348873   50.265274   \n",
       "1   88.04348  73.173910  13.086957  43.173912  50.652172   63.913044   \n",
       "2  107.10411  78.559130  20.420310  51.289204  64.726220  104.325195   \n",
       "3  108.34437  91.423840  31.549670  70.642390  83.642390  126.701990   \n",
       "4   86.17614  54.676136  12.329545  52.948864  67.443184   73.170456   \n",
       "\n",
       "           24  label  \n",
       "0   32.673634    4.0  \n",
       "1   47.347828    7.0  \n",
       "2   83.660670    6.0  \n",
       "3  110.629140    8.0  \n",
       "4   46.937500    4.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82153, 27)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove three columns as index base\n",
    "df2.drop(['field_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.844051</td>\n",
       "      <td>49.840836</td>\n",
       "      <td>62.491962</td>\n",
       "      <td>73.43569</td>\n",
       "      <td>48.863342</td>\n",
       "      <td>13.385852</td>\n",
       "      <td>46.858520</td>\n",
       "      <td>59.025723</td>\n",
       "      <td>73.668810</td>\n",
       "      <td>49.313503</td>\n",
       "      <td>...</td>\n",
       "      <td>41.763664</td>\n",
       "      <td>51.731510</td>\n",
       "      <td>79.36656</td>\n",
       "      <td>58.991962</td>\n",
       "      <td>9.366559</td>\n",
       "      <td>42.319935</td>\n",
       "      <td>50.348873</td>\n",
       "      <td>50.265274</td>\n",
       "      <td>32.673634</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.521740</td>\n",
       "      <td>62.478260</td>\n",
       "      <td>77.782610</td>\n",
       "      <td>94.82609</td>\n",
       "      <td>66.478264</td>\n",
       "      <td>24.304348</td>\n",
       "      <td>60.217392</td>\n",
       "      <td>74.826090</td>\n",
       "      <td>96.869570</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>48.521740</td>\n",
       "      <td>57.913044</td>\n",
       "      <td>88.04348</td>\n",
       "      <td>73.173910</td>\n",
       "      <td>13.086957</td>\n",
       "      <td>43.173912</td>\n",
       "      <td>50.652172</td>\n",
       "      <td>63.913044</td>\n",
       "      <td>47.347828</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.984575</td>\n",
       "      <td>80.282776</td>\n",
       "      <td>99.856040</td>\n",
       "      <td>134.85219</td>\n",
       "      <td>103.096400</td>\n",
       "      <td>13.336761</td>\n",
       "      <td>49.039845</td>\n",
       "      <td>69.282776</td>\n",
       "      <td>111.029564</td>\n",
       "      <td>78.344475</td>\n",
       "      <td>...</td>\n",
       "      <td>50.354755</td>\n",
       "      <td>67.370180</td>\n",
       "      <td>107.10411</td>\n",
       "      <td>78.559130</td>\n",
       "      <td>20.420310</td>\n",
       "      <td>51.289204</td>\n",
       "      <td>64.726220</td>\n",
       "      <td>104.325195</td>\n",
       "      <td>83.660670</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33.019867</td>\n",
       "      <td>72.152320</td>\n",
       "      <td>87.238410</td>\n",
       "      <td>128.99338</td>\n",
       "      <td>111.370860</td>\n",
       "      <td>28.245033</td>\n",
       "      <td>64.211920</td>\n",
       "      <td>78.933780</td>\n",
       "      <td>118.099335</td>\n",
       "      <td>100.072845</td>\n",
       "      <td>...</td>\n",
       "      <td>57.397350</td>\n",
       "      <td>70.059600</td>\n",
       "      <td>108.34437</td>\n",
       "      <td>91.423840</td>\n",
       "      <td>31.549670</td>\n",
       "      <td>70.642390</td>\n",
       "      <td>83.642390</td>\n",
       "      <td>126.701990</td>\n",
       "      <td>110.629140</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.170454</td>\n",
       "      <td>66.335230</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>91.10227</td>\n",
       "      <td>60.363636</td>\n",
       "      <td>18.426136</td>\n",
       "      <td>61.147728</td>\n",
       "      <td>76.948860</td>\n",
       "      <td>90.153410</td>\n",
       "      <td>60.886364</td>\n",
       "      <td>...</td>\n",
       "      <td>63.232956</td>\n",
       "      <td>81.590910</td>\n",
       "      <td>86.17614</td>\n",
       "      <td>54.676136</td>\n",
       "      <td>12.329545</td>\n",
       "      <td>52.948864</td>\n",
       "      <td>67.443184</td>\n",
       "      <td>73.170456</td>\n",
       "      <td>46.937500</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3           4          5  \\\n",
       "0  14.844051  49.840836  62.491962   73.43569   48.863342  13.385852   \n",
       "1  20.521740  62.478260  77.782610   94.82609   66.478264  24.304348   \n",
       "2  29.984575  80.282776  99.856040  134.85219  103.096400  13.336761   \n",
       "3  33.019867  72.152320  87.238410  128.99338  111.370860  28.245033   \n",
       "4  19.170454  66.335230  84.000000   91.10227   60.363636  18.426136   \n",
       "\n",
       "           6          7           8           9  ...         16         17  \\\n",
       "0  46.858520  59.025723   73.668810   49.313503  ...  41.763664  51.731510   \n",
       "1  60.217392  74.826090   96.869570   68.000000  ...  48.521740  57.913044   \n",
       "2  49.039845  69.282776  111.029564   78.344475  ...  50.354755  67.370180   \n",
       "3  64.211920  78.933780  118.099335  100.072845  ...  57.397350  70.059600   \n",
       "4  61.147728  76.948860   90.153410   60.886364  ...  63.232956  81.590910   \n",
       "\n",
       "          18         19         20         21         22          23  \\\n",
       "0   79.36656  58.991962   9.366559  42.319935  50.348873   50.265274   \n",
       "1   88.04348  73.173910  13.086957  43.173912  50.652172   63.913044   \n",
       "2  107.10411  78.559130  20.420310  51.289204  64.726220  104.325195   \n",
       "3  108.34437  91.423840  31.549670  70.642390  83.642390  126.701990   \n",
       "4   86.17614  54.676136  12.329545  52.948864  67.443184   73.170456   \n",
       "\n",
       "           24  label  \n",
       "0   32.673634    4.0  \n",
       "1   47.347828    7.0  \n",
       "2   83.660670    6.0  \n",
       "3  110.629140    8.0  \n",
       "4   46.937500    4.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df1,df2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat(frames,axis =1,ignore_index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.114147</td>\n",
       "      <td>30.607718</td>\n",
       "      <td>58.736336</td>\n",
       "      <td>21.596462</td>\n",
       "      <td>29.223473</td>\n",
       "      <td>57.065918</td>\n",
       "      <td>20.252410</td>\n",
       "      <td>27.516077</td>\n",
       "      <td>50.303860</td>\n",
       "      <td>...</td>\n",
       "      <td>41.763664</td>\n",
       "      <td>51.731510</td>\n",
       "      <td>79.36656</td>\n",
       "      <td>58.991962</td>\n",
       "      <td>9.366559</td>\n",
       "      <td>42.319935</td>\n",
       "      <td>50.348873</td>\n",
       "      <td>50.265274</td>\n",
       "      <td>32.673634</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.043480</td>\n",
       "      <td>46.913044</td>\n",
       "      <td>73.913040</td>\n",
       "      <td>34.391304</td>\n",
       "      <td>46.695652</td>\n",
       "      <td>73.347824</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>36.173912</td>\n",
       "      <td>56.478260</td>\n",
       "      <td>...</td>\n",
       "      <td>48.521740</td>\n",
       "      <td>57.913044</td>\n",
       "      <td>88.04348</td>\n",
       "      <td>73.173910</td>\n",
       "      <td>13.086957</td>\n",
       "      <td>43.173912</td>\n",
       "      <td>50.652172</td>\n",
       "      <td>63.913044</td>\n",
       "      <td>47.347828</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.167095</td>\n",
       "      <td>63.654243</td>\n",
       "      <td>94.773780</td>\n",
       "      <td>23.071980</td>\n",
       "      <td>35.622110</td>\n",
       "      <td>62.772495</td>\n",
       "      <td>27.208227</td>\n",
       "      <td>39.453728</td>\n",
       "      <td>65.179950</td>\n",
       "      <td>...</td>\n",
       "      <td>50.354755</td>\n",
       "      <td>67.370180</td>\n",
       "      <td>107.10411</td>\n",
       "      <td>78.559130</td>\n",
       "      <td>20.420310</td>\n",
       "      <td>51.289204</td>\n",
       "      <td>64.726220</td>\n",
       "      <td>104.325195</td>\n",
       "      <td>83.660670</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>45.847683</td>\n",
       "      <td>61.059600</td>\n",
       "      <td>82.986755</td>\n",
       "      <td>40.158940</td>\n",
       "      <td>54.165560</td>\n",
       "      <td>76.940400</td>\n",
       "      <td>28.092714</td>\n",
       "      <td>39.218544</td>\n",
       "      <td>58.317883</td>\n",
       "      <td>...</td>\n",
       "      <td>57.397350</td>\n",
       "      <td>70.059600</td>\n",
       "      <td>108.34437</td>\n",
       "      <td>91.423840</td>\n",
       "      <td>31.549670</td>\n",
       "      <td>70.642390</td>\n",
       "      <td>83.642390</td>\n",
       "      <td>126.701990</td>\n",
       "      <td>110.629140</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>29.994318</td>\n",
       "      <td>37.738636</td>\n",
       "      <td>80.744316</td>\n",
       "      <td>29.147728</td>\n",
       "      <td>36.630680</td>\n",
       "      <td>73.278410</td>\n",
       "      <td>27.312500</td>\n",
       "      <td>35.318180</td>\n",
       "      <td>71.267044</td>\n",
       "      <td>...</td>\n",
       "      <td>63.232956</td>\n",
       "      <td>81.590910</td>\n",
       "      <td>86.17614</td>\n",
       "      <td>54.676136</td>\n",
       "      <td>12.329545</td>\n",
       "      <td>52.948864</td>\n",
       "      <td>67.443184</td>\n",
       "      <td>73.170456</td>\n",
       "      <td>46.937500</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   field_id          0          1          2          3          4          5  \\\n",
       "0       1.0  23.114147  30.607718  58.736336  21.596462  29.223473  57.065918   \n",
       "1       2.0  32.043480  46.913044  73.913040  34.391304  46.695652  73.347824   \n",
       "2       3.0  45.167095  63.654243  94.773780  23.071980  35.622110  62.772495   \n",
       "3       4.0  45.847683  61.059600  82.986755  40.158940  54.165560  76.940400   \n",
       "4       6.0  29.994318  37.738636  80.744316  29.147728  36.630680  73.278410   \n",
       "\n",
       "           6          7          8  ...         16         17         18  \\\n",
       "0  20.252410  27.516077  50.303860  ...  41.763664  51.731510   79.36656   \n",
       "1  24.000000  36.173912  56.478260  ...  48.521740  57.913044   88.04348   \n",
       "2  27.208227  39.453728  65.179950  ...  50.354755  67.370180  107.10411   \n",
       "3  28.092714  39.218544  58.317883  ...  57.397350  70.059600  108.34437   \n",
       "4  27.312500  35.318180  71.267044  ...  63.232956  81.590910   86.17614   \n",
       "\n",
       "          19         20         21         22          23          24  label  \n",
       "0  58.991962   9.366559  42.319935  50.348873   50.265274   32.673634    4.0  \n",
       "1  73.173910  13.086957  43.173912  50.652172   63.913044   47.347828    7.0  \n",
       "2  78.559130  20.420310  51.289204  64.726220  104.325195   83.660670    6.0  \n",
       "3  91.423840  31.549670  70.642390  83.642390  126.701990  110.629140    8.0  \n",
       "4  54.676136  12.329545  52.948864  67.443184   73.170456   46.937500    4.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82881, 42)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field_id      0\n",
       "0             0\n",
       "1             0\n",
       "2             0\n",
       "3             0\n",
       "4             0\n",
       "5             0\n",
       "6             0\n",
       "7             0\n",
       "8             0\n",
       "9             0\n",
       "10            0\n",
       "11            0\n",
       "12            0\n",
       "13            0\n",
       "14            0\n",
       "0           728\n",
       "1           728\n",
       "2           728\n",
       "3           728\n",
       "4           728\n",
       "5           728\n",
       "6           728\n",
       "7           728\n",
       "8           728\n",
       "9           728\n",
       "10          728\n",
       "11          728\n",
       "12          728\n",
       "13          728\n",
       "14          728\n",
       "15          728\n",
       "16          728\n",
       "17          728\n",
       "18          728\n",
       "19          728\n",
       "20          728\n",
       "21          728\n",
       "22          728\n",
       "23          728\n",
       "24          728\n",
       "label       728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting total null values for each column \n",
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "new_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>23.114147</td>\n",
       "      <td>30.607718</td>\n",
       "      <td>58.736336</td>\n",
       "      <td>21.596462</td>\n",
       "      <td>29.223473</td>\n",
       "      <td>57.065918</td>\n",
       "      <td>20.252410</td>\n",
       "      <td>27.516077</td>\n",
       "      <td>50.303860</td>\n",
       "      <td>...</td>\n",
       "      <td>41.763664</td>\n",
       "      <td>51.731510</td>\n",
       "      <td>79.366560</td>\n",
       "      <td>58.991962</td>\n",
       "      <td>9.366559</td>\n",
       "      <td>42.319935</td>\n",
       "      <td>50.348873</td>\n",
       "      <td>50.265274</td>\n",
       "      <td>32.673634</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>32.043480</td>\n",
       "      <td>46.913044</td>\n",
       "      <td>73.913040</td>\n",
       "      <td>34.391304</td>\n",
       "      <td>46.695652</td>\n",
       "      <td>73.347824</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>36.173912</td>\n",
       "      <td>56.478260</td>\n",
       "      <td>...</td>\n",
       "      <td>48.521740</td>\n",
       "      <td>57.913044</td>\n",
       "      <td>88.043480</td>\n",
       "      <td>73.173910</td>\n",
       "      <td>13.086957</td>\n",
       "      <td>43.173912</td>\n",
       "      <td>50.652172</td>\n",
       "      <td>63.913044</td>\n",
       "      <td>47.347828</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>45.167095</td>\n",
       "      <td>63.654243</td>\n",
       "      <td>94.773780</td>\n",
       "      <td>23.071980</td>\n",
       "      <td>35.622110</td>\n",
       "      <td>62.772495</td>\n",
       "      <td>27.208227</td>\n",
       "      <td>39.453728</td>\n",
       "      <td>65.179950</td>\n",
       "      <td>...</td>\n",
       "      <td>50.354755</td>\n",
       "      <td>67.370180</td>\n",
       "      <td>107.104110</td>\n",
       "      <td>78.559130</td>\n",
       "      <td>20.420310</td>\n",
       "      <td>51.289204</td>\n",
       "      <td>64.726220</td>\n",
       "      <td>104.325195</td>\n",
       "      <td>83.660670</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>45.847683</td>\n",
       "      <td>61.059600</td>\n",
       "      <td>82.986755</td>\n",
       "      <td>40.158940</td>\n",
       "      <td>54.165560</td>\n",
       "      <td>76.940400</td>\n",
       "      <td>28.092714</td>\n",
       "      <td>39.218544</td>\n",
       "      <td>58.317883</td>\n",
       "      <td>...</td>\n",
       "      <td>57.397350</td>\n",
       "      <td>70.059600</td>\n",
       "      <td>108.344370</td>\n",
       "      <td>91.423840</td>\n",
       "      <td>31.549670</td>\n",
       "      <td>70.642390</td>\n",
       "      <td>83.642390</td>\n",
       "      <td>126.701990</td>\n",
       "      <td>110.629140</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>29.994318</td>\n",
       "      <td>37.738636</td>\n",
       "      <td>80.744316</td>\n",
       "      <td>29.147728</td>\n",
       "      <td>36.630680</td>\n",
       "      <td>73.278410</td>\n",
       "      <td>27.312500</td>\n",
       "      <td>35.318180</td>\n",
       "      <td>71.267044</td>\n",
       "      <td>...</td>\n",
       "      <td>63.232956</td>\n",
       "      <td>81.590910</td>\n",
       "      <td>86.176140</td>\n",
       "      <td>54.676136</td>\n",
       "      <td>12.329545</td>\n",
       "      <td>52.948864</td>\n",
       "      <td>67.443184</td>\n",
       "      <td>73.170456</td>\n",
       "      <td>46.937500</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82148</th>\n",
       "      <td>121646.0</td>\n",
       "      <td>39.309975</td>\n",
       "      <td>55.024258</td>\n",
       "      <td>77.630730</td>\n",
       "      <td>35.738544</td>\n",
       "      <td>50.671158</td>\n",
       "      <td>73.002690</td>\n",
       "      <td>35.148247</td>\n",
       "      <td>51.247980</td>\n",
       "      <td>73.606470</td>\n",
       "      <td>...</td>\n",
       "      <td>49.505127</td>\n",
       "      <td>59.982050</td>\n",
       "      <td>83.582054</td>\n",
       "      <td>57.202564</td>\n",
       "      <td>15.035897</td>\n",
       "      <td>49.066666</td>\n",
       "      <td>59.153847</td>\n",
       "      <td>85.764100</td>\n",
       "      <td>59.712822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82149</th>\n",
       "      <td>121647.0</td>\n",
       "      <td>15.902778</td>\n",
       "      <td>23.337963</td>\n",
       "      <td>39.824074</td>\n",
       "      <td>20.328703</td>\n",
       "      <td>29.875000</td>\n",
       "      <td>51.763890</td>\n",
       "      <td>19.222221</td>\n",
       "      <td>28.074074</td>\n",
       "      <td>50.796295</td>\n",
       "      <td>...</td>\n",
       "      <td>54.503677</td>\n",
       "      <td>68.650734</td>\n",
       "      <td>79.216910</td>\n",
       "      <td>52.647060</td>\n",
       "      <td>8.948529</td>\n",
       "      <td>47.496323</td>\n",
       "      <td>60.220590</td>\n",
       "      <td>75.485290</td>\n",
       "      <td>51.580883</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82150</th>\n",
       "      <td>121649.0</td>\n",
       "      <td>42.666668</td>\n",
       "      <td>65.523810</td>\n",
       "      <td>94.476190</td>\n",
       "      <td>45.142857</td>\n",
       "      <td>68.904760</td>\n",
       "      <td>97.238100</td>\n",
       "      <td>37.380950</td>\n",
       "      <td>59.285713</td>\n",
       "      <td>86.428570</td>\n",
       "      <td>...</td>\n",
       "      <td>72.145380</td>\n",
       "      <td>78.350950</td>\n",
       "      <td>118.063140</td>\n",
       "      <td>101.806170</td>\n",
       "      <td>16.491924</td>\n",
       "      <td>72.201170</td>\n",
       "      <td>78.311310</td>\n",
       "      <td>116.662260</td>\n",
       "      <td>100.261380</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82151</th>\n",
       "      <td>121650.0</td>\n",
       "      <td>38.218750</td>\n",
       "      <td>60.468750</td>\n",
       "      <td>84.015625</td>\n",
       "      <td>29.945312</td>\n",
       "      <td>54.710938</td>\n",
       "      <td>76.960940</td>\n",
       "      <td>27.835938</td>\n",
       "      <td>50.437500</td>\n",
       "      <td>71.304690</td>\n",
       "      <td>...</td>\n",
       "      <td>46.615383</td>\n",
       "      <td>59.769230</td>\n",
       "      <td>59.507690</td>\n",
       "      <td>37.846153</td>\n",
       "      <td>9.830770</td>\n",
       "      <td>47.553844</td>\n",
       "      <td>62.569230</td>\n",
       "      <td>64.907690</td>\n",
       "      <td>41.738460</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82152</th>\n",
       "      <td>121652.0</td>\n",
       "      <td>60.335630</td>\n",
       "      <td>78.351280</td>\n",
       "      <td>98.926110</td>\n",
       "      <td>59.335630</td>\n",
       "      <td>77.910450</td>\n",
       "      <td>99.644960</td>\n",
       "      <td>60.672512</td>\n",
       "      <td>79.550410</td>\n",
       "      <td>101.443330</td>\n",
       "      <td>...</td>\n",
       "      <td>91.158295</td>\n",
       "      <td>99.249420</td>\n",
       "      <td>128.327240</td>\n",
       "      <td>113.720320</td>\n",
       "      <td>22.477552</td>\n",
       "      <td>80.153980</td>\n",
       "      <td>88.867645</td>\n",
       "      <td>114.459595</td>\n",
       "      <td>95.780180</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82153 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       field_id          0          1          2          3          4  \\\n",
       "0           1.0  23.114147  30.607718  58.736336  21.596462  29.223473   \n",
       "1           2.0  32.043480  46.913044  73.913040  34.391304  46.695652   \n",
       "2           3.0  45.167095  63.654243  94.773780  23.071980  35.622110   \n",
       "3           4.0  45.847683  61.059600  82.986755  40.158940  54.165560   \n",
       "4           6.0  29.994318  37.738636  80.744316  29.147728  36.630680   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "82148  121646.0  39.309975  55.024258  77.630730  35.738544  50.671158   \n",
       "82149  121647.0  15.902778  23.337963  39.824074  20.328703  29.875000   \n",
       "82150  121649.0  42.666668  65.523810  94.476190  45.142857  68.904760   \n",
       "82151  121650.0  38.218750  60.468750  84.015625  29.945312  54.710938   \n",
       "82152  121652.0  60.335630  78.351280  98.926110  59.335630  77.910450   \n",
       "\n",
       "               5          6          7           8  ...         16         17  \\\n",
       "0      57.065918  20.252410  27.516077   50.303860  ...  41.763664  51.731510   \n",
       "1      73.347824  24.000000  36.173912   56.478260  ...  48.521740  57.913044   \n",
       "2      62.772495  27.208227  39.453728   65.179950  ...  50.354755  67.370180   \n",
       "3      76.940400  28.092714  39.218544   58.317883  ...  57.397350  70.059600   \n",
       "4      73.278410  27.312500  35.318180   71.267044  ...  63.232956  81.590910   \n",
       "...          ...        ...        ...         ...  ...        ...        ...   \n",
       "82148  73.002690  35.148247  51.247980   73.606470  ...  49.505127  59.982050   \n",
       "82149  51.763890  19.222221  28.074074   50.796295  ...  54.503677  68.650734   \n",
       "82150  97.238100  37.380950  59.285713   86.428570  ...  72.145380  78.350950   \n",
       "82151  76.960940  27.835938  50.437500   71.304690  ...  46.615383  59.769230   \n",
       "82152  99.644960  60.672512  79.550410  101.443330  ...  91.158295  99.249420   \n",
       "\n",
       "               18          19         20         21         22          23  \\\n",
       "0       79.366560   58.991962   9.366559  42.319935  50.348873   50.265274   \n",
       "1       88.043480   73.173910  13.086957  43.173912  50.652172   63.913044   \n",
       "2      107.104110   78.559130  20.420310  51.289204  64.726220  104.325195   \n",
       "3      108.344370   91.423840  31.549670  70.642390  83.642390  126.701990   \n",
       "4       86.176140   54.676136  12.329545  52.948864  67.443184   73.170456   \n",
       "...           ...         ...        ...        ...        ...         ...   \n",
       "82148   83.582054   57.202564  15.035897  49.066666  59.153847   85.764100   \n",
       "82149   79.216910   52.647060   8.948529  47.496323  60.220590   75.485290   \n",
       "82150  118.063140  101.806170  16.491924  72.201170  78.311310  116.662260   \n",
       "82151   59.507690   37.846153   9.830770  47.553844  62.569230   64.907690   \n",
       "82152  128.327240  113.720320  22.477552  80.153980  88.867645  114.459595   \n",
       "\n",
       "               24  label  \n",
       "0       32.673634    4.0  \n",
       "1       47.347828    7.0  \n",
       "2       83.660670    6.0  \n",
       "3      110.629140    8.0  \n",
       "4       46.937500    4.0  \n",
       "...           ...    ...  \n",
       "82148   59.712822    1.0  \n",
       "82149   51.580883    4.0  \n",
       "82150  100.261380    5.0  \n",
       "82151   41.738460    2.0  \n",
       "82152   95.780180    9.0  \n",
       "\n",
       "[82153 rows x 42 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['field_id', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "       '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
       "       '23', '24', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.columns.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field_id    0\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "5           0\n",
       "6           0\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          0\n",
       "12          0\n",
       "13          0\n",
       "14          0\n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           0\n",
       "5           0\n",
       "6           0\n",
       "7           0\n",
       "8           0\n",
       "9           0\n",
       "10          0\n",
       "11          0\n",
       "12          0\n",
       "13          0\n",
       "14          0\n",
       "15          0\n",
       "16          0\n",
       "17          0\n",
       "18          0\n",
       "19          0\n",
       "20          0\n",
       "21          0\n",
       "22          0\n",
       "23          0\n",
       "24          0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting total null values for each column \n",
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('new_data_merged1.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14.844051</td>\n",
       "      <td>49.840836</td>\n",
       "      <td>62.491962</td>\n",
       "      <td>73.435690</td>\n",
       "      <td>48.863342</td>\n",
       "      <td>13.385852</td>\n",
       "      <td>46.858520</td>\n",
       "      <td>59.025723</td>\n",
       "      <td>73.668810</td>\n",
       "      <td>...</td>\n",
       "      <td>41.763664</td>\n",
       "      <td>51.731510</td>\n",
       "      <td>79.366560</td>\n",
       "      <td>58.991962</td>\n",
       "      <td>9.366559</td>\n",
       "      <td>42.319935</td>\n",
       "      <td>50.348873</td>\n",
       "      <td>50.265274</td>\n",
       "      <td>32.673634</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20.521740</td>\n",
       "      <td>62.478260</td>\n",
       "      <td>77.782610</td>\n",
       "      <td>94.826090</td>\n",
       "      <td>66.478264</td>\n",
       "      <td>24.304348</td>\n",
       "      <td>60.217392</td>\n",
       "      <td>74.826090</td>\n",
       "      <td>96.869570</td>\n",
       "      <td>...</td>\n",
       "      <td>48.521740</td>\n",
       "      <td>57.913044</td>\n",
       "      <td>88.043480</td>\n",
       "      <td>73.173910</td>\n",
       "      <td>13.086957</td>\n",
       "      <td>43.173912</td>\n",
       "      <td>50.652172</td>\n",
       "      <td>63.913044</td>\n",
       "      <td>47.347828</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>29.984575</td>\n",
       "      <td>80.282776</td>\n",
       "      <td>99.856040</td>\n",
       "      <td>134.852190</td>\n",
       "      <td>103.096400</td>\n",
       "      <td>13.336761</td>\n",
       "      <td>49.039845</td>\n",
       "      <td>69.282776</td>\n",
       "      <td>111.029564</td>\n",
       "      <td>...</td>\n",
       "      <td>50.354755</td>\n",
       "      <td>67.370180</td>\n",
       "      <td>107.104110</td>\n",
       "      <td>78.559130</td>\n",
       "      <td>20.420310</td>\n",
       "      <td>51.289204</td>\n",
       "      <td>64.726220</td>\n",
       "      <td>104.325195</td>\n",
       "      <td>83.660670</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>33.019867</td>\n",
       "      <td>72.152320</td>\n",
       "      <td>87.238410</td>\n",
       "      <td>128.993380</td>\n",
       "      <td>111.370860</td>\n",
       "      <td>28.245033</td>\n",
       "      <td>64.211920</td>\n",
       "      <td>78.933780</td>\n",
       "      <td>118.099335</td>\n",
       "      <td>...</td>\n",
       "      <td>57.397350</td>\n",
       "      <td>70.059600</td>\n",
       "      <td>108.344370</td>\n",
       "      <td>91.423840</td>\n",
       "      <td>31.549670</td>\n",
       "      <td>70.642390</td>\n",
       "      <td>83.642390</td>\n",
       "      <td>126.701990</td>\n",
       "      <td>110.629140</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>19.170454</td>\n",
       "      <td>66.335230</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>91.102270</td>\n",
       "      <td>60.363636</td>\n",
       "      <td>18.426136</td>\n",
       "      <td>61.147728</td>\n",
       "      <td>76.948860</td>\n",
       "      <td>90.153410</td>\n",
       "      <td>...</td>\n",
       "      <td>63.232956</td>\n",
       "      <td>81.590910</td>\n",
       "      <td>86.176140</td>\n",
       "      <td>54.676136</td>\n",
       "      <td>12.329545</td>\n",
       "      <td>52.948864</td>\n",
       "      <td>67.443184</td>\n",
       "      <td>73.170456</td>\n",
       "      <td>46.937500</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82148</th>\n",
       "      <td>122729.0</td>\n",
       "      <td>20.456410</td>\n",
       "      <td>54.507690</td>\n",
       "      <td>66.569230</td>\n",
       "      <td>78.435900</td>\n",
       "      <td>50.417950</td>\n",
       "      <td>19.769230</td>\n",
       "      <td>54.982050</td>\n",
       "      <td>66.971794</td>\n",
       "      <td>93.871796</td>\n",
       "      <td>...</td>\n",
       "      <td>49.505127</td>\n",
       "      <td>59.982050</td>\n",
       "      <td>83.582054</td>\n",
       "      <td>57.202564</td>\n",
       "      <td>15.035897</td>\n",
       "      <td>49.066666</td>\n",
       "      <td>59.153847</td>\n",
       "      <td>85.764100</td>\n",
       "      <td>59.712822</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82149</th>\n",
       "      <td>122731.0</td>\n",
       "      <td>14.084558</td>\n",
       "      <td>67.389710</td>\n",
       "      <td>83.500000</td>\n",
       "      <td>84.599266</td>\n",
       "      <td>56.930145</td>\n",
       "      <td>12.875000</td>\n",
       "      <td>64.404410</td>\n",
       "      <td>79.349266</td>\n",
       "      <td>87.522060</td>\n",
       "      <td>...</td>\n",
       "      <td>54.503677</td>\n",
       "      <td>68.650734</td>\n",
       "      <td>79.216910</td>\n",
       "      <td>52.647060</td>\n",
       "      <td>8.948529</td>\n",
       "      <td>47.496323</td>\n",
       "      <td>60.220590</td>\n",
       "      <td>75.485290</td>\n",
       "      <td>51.580883</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82150</th>\n",
       "      <td>122732.0</td>\n",
       "      <td>20.218796</td>\n",
       "      <td>71.324524</td>\n",
       "      <td>77.587370</td>\n",
       "      <td>117.635826</td>\n",
       "      <td>100.364174</td>\n",
       "      <td>18.048458</td>\n",
       "      <td>71.400880</td>\n",
       "      <td>77.487520</td>\n",
       "      <td>116.284874</td>\n",
       "      <td>...</td>\n",
       "      <td>72.145380</td>\n",
       "      <td>78.350950</td>\n",
       "      <td>118.063140</td>\n",
       "      <td>101.806170</td>\n",
       "      <td>16.491924</td>\n",
       "      <td>72.201170</td>\n",
       "      <td>78.311310</td>\n",
       "      <td>116.662260</td>\n",
       "      <td>100.261380</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82151</th>\n",
       "      <td>122733.0</td>\n",
       "      <td>13.261539</td>\n",
       "      <td>43.584614</td>\n",
       "      <td>55.938460</td>\n",
       "      <td>65.292305</td>\n",
       "      <td>44.030770</td>\n",
       "      <td>10.676923</td>\n",
       "      <td>43.738460</td>\n",
       "      <td>57.307693</td>\n",
       "      <td>71.661540</td>\n",
       "      <td>...</td>\n",
       "      <td>46.615383</td>\n",
       "      <td>59.769230</td>\n",
       "      <td>59.507690</td>\n",
       "      <td>37.846153</td>\n",
       "      <td>9.830770</td>\n",
       "      <td>47.553844</td>\n",
       "      <td>62.569230</td>\n",
       "      <td>64.907690</td>\n",
       "      <td>41.738460</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82152</th>\n",
       "      <td>122736.0</td>\n",
       "      <td>25.023611</td>\n",
       "      <td>89.103424</td>\n",
       "      <td>97.183910</td>\n",
       "      <td>128.842700</td>\n",
       "      <td>115.675095</td>\n",
       "      <td>23.107416</td>\n",
       "      <td>88.125040</td>\n",
       "      <td>96.191220</td>\n",
       "      <td>124.454940</td>\n",
       "      <td>...</td>\n",
       "      <td>91.158295</td>\n",
       "      <td>99.249420</td>\n",
       "      <td>128.327240</td>\n",
       "      <td>113.720320</td>\n",
       "      <td>22.477552</td>\n",
       "      <td>80.153980</td>\n",
       "      <td>88.867645</td>\n",
       "      <td>114.459595</td>\n",
       "      <td>95.780180</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82153 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       field_id          0          1          2           3           4  \\\n",
       "0           1.0  14.844051  49.840836  62.491962   73.435690   48.863342   \n",
       "1           2.0  20.521740  62.478260  77.782610   94.826090   66.478264   \n",
       "2           3.0  29.984575  80.282776  99.856040  134.852190  103.096400   \n",
       "3           4.0  33.019867  72.152320  87.238410  128.993380  111.370860   \n",
       "4           6.0  19.170454  66.335230  84.000000   91.102270   60.363636   \n",
       "...         ...        ...        ...        ...         ...         ...   \n",
       "82148  122729.0  20.456410  54.507690  66.569230   78.435900   50.417950   \n",
       "82149  122731.0  14.084558  67.389710  83.500000   84.599266   56.930145   \n",
       "82150  122732.0  20.218796  71.324524  77.587370  117.635826  100.364174   \n",
       "82151  122733.0  13.261539  43.584614  55.938460   65.292305   44.030770   \n",
       "82152  122736.0  25.023611  89.103424  97.183910  128.842700  115.675095   \n",
       "\n",
       "               5          6          7           8  ...         16         17  \\\n",
       "0      13.385852  46.858520  59.025723   73.668810  ...  41.763664  51.731510   \n",
       "1      24.304348  60.217392  74.826090   96.869570  ...  48.521740  57.913044   \n",
       "2      13.336761  49.039845  69.282776  111.029564  ...  50.354755  67.370180   \n",
       "3      28.245033  64.211920  78.933780  118.099335  ...  57.397350  70.059600   \n",
       "4      18.426136  61.147728  76.948860   90.153410  ...  63.232956  81.590910   \n",
       "...          ...        ...        ...         ...  ...        ...        ...   \n",
       "82148  19.769230  54.982050  66.971794   93.871796  ...  49.505127  59.982050   \n",
       "82149  12.875000  64.404410  79.349266   87.522060  ...  54.503677  68.650734   \n",
       "82150  18.048458  71.400880  77.487520  116.284874  ...  72.145380  78.350950   \n",
       "82151  10.676923  43.738460  57.307693   71.661540  ...  46.615383  59.769230   \n",
       "82152  23.107416  88.125040  96.191220  124.454940  ...  91.158295  99.249420   \n",
       "\n",
       "               18          19         20         21         22          23  \\\n",
       "0       79.366560   58.991962   9.366559  42.319935  50.348873   50.265274   \n",
       "1       88.043480   73.173910  13.086957  43.173912  50.652172   63.913044   \n",
       "2      107.104110   78.559130  20.420310  51.289204  64.726220  104.325195   \n",
       "3      108.344370   91.423840  31.549670  70.642390  83.642390  126.701990   \n",
       "4       86.176140   54.676136  12.329545  52.948864  67.443184   73.170456   \n",
       "...           ...         ...        ...        ...        ...         ...   \n",
       "82148   83.582054   57.202564  15.035897  49.066666  59.153847   85.764100   \n",
       "82149   79.216910   52.647060   8.948529  47.496323  60.220590   75.485290   \n",
       "82150  118.063140  101.806170  16.491924  72.201170  78.311310  116.662260   \n",
       "82151   59.507690   37.846153   9.830770  47.553844  62.569230   64.907690   \n",
       "82152  128.327240  113.720320  22.477552  80.153980  88.867645  114.459595   \n",
       "\n",
       "               24  label  \n",
       "0       32.673634    4.0  \n",
       "1       47.347828    7.0  \n",
       "2       83.660670    6.0  \n",
       "3      110.629140    8.0  \n",
       "4       46.937500    4.0  \n",
       "...           ...    ...  \n",
       "82148   59.712822    1.0  \n",
       "82149   51.580883    4.0  \n",
       "82150  100.261380    5.0  \n",
       "82151   41.738460    2.0  \n",
       "82152   95.780180    9.0  \n",
       "\n",
       "[82153 rows x 27 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from flaml import AutoML\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, balanced_accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to plot confusion matrix \n",
    "# plot confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(y_test, y_pred , model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function takes y_test, y_pred and model_name from the machine learning models\n",
    "    Returns a plot of confusion matrix with labels marked  \n",
    "    \"\"\"\n",
    "    \n",
    "    # create confusion matrix using the y_pre and y_test\n",
    "    cm = confusion_matrix(y_pred, y_test)\n",
    "    \n",
    "    # set columns\n",
    "    classes_unique = y_train.unique()\n",
    "    columns = sorted(classes_unique)\n",
    "    \n",
    "    #Create a plot instance\n",
    "    fig, ax = plt.subplots(figsize = (20,20))\n",
    "    df_cm = pd.DataFrame(cm, index = columns, columns = columns)\n",
    "\n",
    "    ax = sns.heatmap(df_cm, cmap='nipy_spectral_r', annot=True,  annot_kws={\"size\": 15},  fmt=\".1f\")\n",
    "    \n",
    "     \n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(len(y_test.unique())))\n",
    "    ax.set_yticks(np.arange(len(y_test.unique())))\n",
    "    \n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(sorted(y_test.unique()))\n",
    "    ax.set_yticklabels(sorted(y_test.unique()))\n",
    "    \n",
    "    #Set labels\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    \n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\", va = \"center_baseline\", rotation_mode=\"anchor\")\n",
    "\n",
    "#     # Loop over data dimensions and create text annotations.\n",
    "#     for i in range(len(cm)):\n",
    "#         for j in range(len(cm)):\n",
    "#             text = ax.text(j, i, cm[i, j],\n",
    "#                            ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "        \n",
    "    ax.set_title(\"Confusion Matrix for \"+model_name+\" Model\")\n",
    "    sns.set(font_scale=1.6)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to estimate model scores \n",
    "results_df = pd.DataFrame(columns = ['Model','Balanced Accuracy','Accuracy','Precision','Recall',\n",
    "                                    'F1 Score','Train Score','Test Score','Best_params'])\n",
    "\n",
    "def calculate_scores(y_test, y_pred, train_score, test_score, best_params, model, df):\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred , average = 'weighted')\n",
    "    rec = recall_score(y_test, y_pred, average = 'weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average = 'weighted')\n",
    "    train_Score = train_score\n",
    "    test_score = test_score\n",
    "    best_params = best_params\n",
    "    \n",
    "    result_list = [model, balanced_accuracy, accuracy, prec, rec, f1, train_score, test_score, best_params]\n",
    "    \n",
    "    df.loc[len(df)] = result_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store land use classes name sorted by land use class_id\n",
    "lc_class_name_list = data.groupby('label')['label'].apply(lambda x: list(np.unique(x)))\n",
    "lc_class_name_list = lc_class_name_list.explode('label').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_class_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lc_class_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "# We use field_ids to split the data to train and test. Note that the test portion for training is different than the test \n",
    "# portion provided as part of the competition. \n",
    "train_per = 0.7\n",
    "\n",
    "n_fields = len(data['field_id'])\n",
    "np.random.seed(10)\n",
    "train_fields = np.random.choice(data['field_id'], int(n_fields * train_per), replace=False)\n",
    "test_fields = data['field_id'][~np.in1d(data['field_id'], train_fields)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = data[data['field_id'].isin(train_fields)], data[data['field_id'].isin(test_fields)]\n",
    "X_train = X_train.drop(columns=['label', 'field_id'])\n",
    "X_test = X_test.drop(columns=['label', 'field_id'])\n",
    "y_train, y_test = data[data['field_id'].isin(train_fields)]['label'], data[data['field_id'].isin(test_fields)]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57507\n",
      "24646\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-0b052d41b57a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0    16583\n",
       "2.0     9345\n",
       "7.0     6980\n",
       "1.0     5540\n",
       "6.0     5301\n",
       "5.0     5201\n",
       "3.0     5077\n",
       "9.0     2477\n",
       "8.0     1003\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Number of data points in training data \n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"time_budget\": 2000,  # total running time in seconds\n",
    "    \"metric\": 'accuracy',  # can be: 'r2', 'rmse', 'mae', 'mse', 'accuracy', 'roc_auc', 'roc_auc_ovr',\n",
    "                           # 'roc_auc_ovo', 'log_loss', 'mape', 'f1', 'ap', 'ndcg', 'micro_f1', 'macro_f1'\n",
    "    \"task\": 'classification',  # task type    \n",
    "    \"log_file_name\": 'classification1.log',  # flaml log file\n",
    "    \"seed\": 123,    # random seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 10-01 12:49:30] {1432} INFO - Evaluation method: cv\n",
      "[flaml.automl: 10-01 12:49:30] {1478} INFO - Minimizing error metric: 1-accuracy\n",
      "[flaml.automl: 10-01 12:49:30] {1515} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'catboost', 'xgboost', 'extra_tree', 'lrl1']\n",
      "[flaml.automl: 10-01 12:49:31] {1748} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:32] {1866} INFO - Estimated sufficient time budget=10994s. Estimated necessary time budget=200s.\n",
      "[flaml.automl: 10-01 12:49:32] {1944} INFO -  at 2.0s,\testimator lgbm's best error=0.6697,\tbest estimator lgbm's best error=0.6697\n",
      "[flaml.automl: 10-01 12:49:32] {1748} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:33] {1944} INFO -  at 3.3s,\testimator lgbm's best error=0.6165,\tbest estimator lgbm's best error=0.6165\n",
      "[flaml.automl: 10-01 12:49:33] {1748} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:34] {1944} INFO -  at 4.7s,\testimator lgbm's best error=0.5865,\tbest estimator lgbm's best error=0.5865\n",
      "[flaml.automl: 10-01 12:49:34] {1748} INFO - iteration 3, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:35] {1944} INFO -  at 5.7s,\testimator lgbm's best error=0.5865,\tbest estimator lgbm's best error=0.5865\n",
      "[flaml.automl: 10-01 12:49:35] {1748} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:38] {1944} INFO -  at 8.6s,\testimator lgbm's best error=0.5284,\tbest estimator lgbm's best error=0.5284\n",
      "[flaml.automl: 10-01 12:49:38] {1748} INFO - iteration 5, current learner xgboost\n",
      "[flaml.automl: 10-01 12:49:40] {1944} INFO -  at 10.1s,\testimator xgboost's best error=0.5791,\tbest estimator lgbm's best error=0.5284\n",
      "[flaml.automl: 10-01 12:49:40] {1748} INFO - iteration 6, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:44] {1944} INFO -  at 14.2s,\testimator lgbm's best error=0.5149,\tbest estimator lgbm's best error=0.5149\n",
      "[flaml.automl: 10-01 12:49:44] {1748} INFO - iteration 7, current learner lgbm\n",
      "[flaml.automl: 10-01 12:49:51] {1944} INFO -  at 21.3s,\testimator lgbm's best error=0.4893,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:49:51] {1748} INFO - iteration 8, current learner xgboost\n",
      "[flaml.automl: 10-01 12:49:53] {1944} INFO -  at 23.8s,\testimator xgboost's best error=0.5173,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:49:53] {1748} INFO - iteration 9, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:49:55] {1944} INFO -  at 25.4s,\testimator extra_tree's best error=0.6314,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:49:55] {1748} INFO - iteration 10, current learner xgboost\n",
      "[flaml.automl: 10-01 12:49:57] {1944} INFO -  at 27.4s,\testimator xgboost's best error=0.5173,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:49:57] {1748} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:49:59] {1944} INFO -  at 29.0s,\testimator extra_tree's best error=0.6029,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:49:59] {1748} INFO - iteration 12, current learner rf\n",
      "[flaml.automl: 10-01 12:50:07] {1944} INFO -  at 37.9s,\testimator rf's best error=0.6249,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:50:07] {1748} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl: 10-01 12:50:10] {1944} INFO -  at 40.2s,\testimator xgboost's best error=0.5173,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:50:10] {1748} INFO - iteration 14, current learner rf\n",
      "[flaml.automl: 10-01 12:50:15] {1944} INFO -  at 45.1s,\testimator rf's best error=0.5988,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:50:15] {1748} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl: 10-01 12:50:18] {1944} INFO -  at 48.5s,\testimator xgboost's best error=0.5093,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:50:18] {1748} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl: 10-01 12:50:20] {1944} INFO -  at 50.6s,\testimator lgbm's best error=0.4893,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:50:20] {1748} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:50:22] {1944} INFO -  at 52.3s,\testimator extra_tree's best error=0.6029,\tbest estimator lgbm's best error=0.4893\n",
      "[flaml.automl: 10-01 12:50:22] {1748} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl: 10-01 12:50:36] {1944} INFO -  at 66.6s,\testimator lgbm's best error=0.4790,\tbest estimator lgbm's best error=0.4790\n",
      "[flaml.automl: 10-01 12:50:36] {1748} INFO - iteration 19, current learner xgboost\n",
      "[flaml.automl: 10-01 12:50:57] {1944} INFO -  at 87.0s,\testimator xgboost's best error=0.4756,\tbest estimator xgboost's best error=0.4756\n",
      "[flaml.automl: 10-01 12:50:57] {1748} INFO - iteration 20, current learner rf\n",
      "[flaml.automl: 10-01 12:51:00] {1944} INFO -  at 90.5s,\testimator rf's best error=0.5988,\tbest estimator xgboost's best error=0.4756\n",
      "[flaml.automl: 10-01 12:51:00] {1748} INFO - iteration 21, current learner xgboost\n",
      "[flaml.automl: 10-01 12:51:03] {1944} INFO -  at 93.8s,\testimator xgboost's best error=0.4756,\tbest estimator xgboost's best error=0.4756\n",
      "[flaml.automl: 10-01 12:51:03] {1748} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl: 10-01 12:53:46] {1944} INFO -  at 256.1s,\testimator lgbm's best error=0.4462,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:53:46] {1748} INFO - iteration 23, current learner catboost\n",
      "[flaml.automl: 10-01 12:55:47] {1944} INFO -  at 377.7s,\testimator catboost's best error=0.4666,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:55:47] {1748} INFO - iteration 24, current learner rf\n",
      "[flaml.automl: 10-01 12:55:52] {1944} INFO -  at 382.5s,\testimator rf's best error=0.5859,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:55:52] {1748} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:55:54] {1944} INFO -  at 384.1s,\testimator extra_tree's best error=0.5929,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:55:54] {1748} INFO - iteration 26, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:55:56] {1944} INFO -  at 386.1s,\testimator extra_tree's best error=0.5929,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:55:56] {1748} INFO - iteration 27, current learner catboost\n",
      "[flaml.automl: 10-01 12:57:09] {1944} INFO -  at 459.6s,\testimator catboost's best error=0.4666,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:57:09] {1748} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl: 10-01 12:57:52] {1944} INFO -  at 502.2s,\testimator xgboost's best error=0.4628,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:57:52] {1748} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:57:53] {1944} INFO -  at 503.8s,\testimator extra_tree's best error=0.5874,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:57:53] {1748} INFO - iteration 30, current learner extra_tree\n",
      "[flaml.automl: 10-01 12:57:56] {1944} INFO -  at 506.1s,\testimator extra_tree's best error=0.5874,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:57:56] {1748} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl: 10-01 12:58:07] {1944} INFO -  at 517.1s,\testimator lgbm's best error=0.4462,\tbest estimator lgbm's best error=0.4462\n",
      "[flaml.automl: 10-01 12:58:07] {1748} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl: 10-01 13:03:39] {1944} INFO -  at 849.0s,\testimator lgbm's best error=0.4429,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:03:39] {1748} INFO - iteration 33, current learner rf\n",
      "[flaml.automl: 10-01 13:03:58] {1944} INFO -  at 868.9s,\testimator rf's best error=0.5859,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:03:58] {1748} INFO - iteration 34, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:04:00] {1944} INFO -  at 870.4s,\testimator extra_tree's best error=0.5874,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:04:00] {1748} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl: 10-01 13:06:50] {1944} INFO -  at 1040.5s,\testimator lgbm's best error=0.4429,\tbest estimator lgbm's best error=0.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 10-01 13:06:50] {1748} INFO - iteration 36, current learner catboost\n",
      "[flaml.automl: 10-01 13:10:21] {1944} INFO -  at 1251.8s,\testimator catboost's best error=0.4666,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:10:21] {1748} INFO - iteration 37, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:10:23] {1944} INFO -  at 1253.9s,\testimator extra_tree's best error=0.5874,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:10:23] {1748} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl: 10-01 13:10:42] {1944} INFO -  at 1272.3s,\testimator xgboost's best error=0.4628,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:10:42] {1748} INFO - iteration 39, current learner lrl1\n",
      "No low-cost partial config given to the search algorithm. For cost-frugal search, consider providing low-cost values for cost-related hps via 'low_cost_partial_config'.\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "[flaml.automl: 10-01 13:11:55] {1944} INFO -  at 1345.7s,\testimator lrl1's best error=0.5226,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:11:55] {1748} INFO - iteration 40, current learner lrl1\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  ConvergenceWarning,\n",
      "[flaml.automl: 10-01 13:13:08] {1944} INFO -  at 1418.7s,\testimator lrl1's best error=0.5225,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:13:08] {1748} INFO - iteration 41, current learner catboost\n",
      "[flaml.automl: 10-01 13:15:11] {1944} INFO -  at 1541.4s,\testimator catboost's best error=0.4666,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:15:11] {1748} INFO - iteration 42, current learner xgboost\n",
      "[flaml.automl: 10-01 13:16:34] {1944} INFO -  at 1624.2s,\testimator xgboost's best error=0.4576,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:16:34] {1748} INFO - iteration 43, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:16:36] {1944} INFO -  at 1625.9s,\testimator extra_tree's best error=0.5542,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:16:36] {1748} INFO - iteration 44, current learner lgbm\n",
      "[flaml.automl: 10-01 13:20:43] {1944} INFO -  at 1873.3s,\testimator lgbm's best error=0.4429,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:43] {1748} INFO - iteration 45, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:20:44] {1944} INFO -  at 1874.8s,\testimator extra_tree's best error=0.5542,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:44] {1748} INFO - iteration 46, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:20:47] {1944} INFO -  at 1877.1s,\testimator extra_tree's best error=0.5388,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:47] {1748} INFO - iteration 47, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:20:48] {1944} INFO -  at 1878.8s,\testimator extra_tree's best error=0.5388,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:48] {1748} INFO - iteration 48, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:20:50] {1944} INFO -  at 1880.5s,\testimator extra_tree's best error=0.5388,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:50] {1748} INFO - iteration 49, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:20:53] {1944} INFO -  at 1883.0s,\testimator extra_tree's best error=0.5272,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:53] {1748} INFO - iteration 50, current learner rf\n",
      "[flaml.automl: 10-01 13:20:58] {1944} INFO -  at 1888.0s,\testimator rf's best error=0.5824,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:58] {1748} INFO - iteration 51, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:20:59] {1944} INFO -  at 1889.7s,\testimator extra_tree's best error=0.5272,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:20:59] {1748} INFO - iteration 52, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:21:03] {1944} INFO -  at 1893.5s,\testimator extra_tree's best error=0.4984,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:21:03] {1748} INFO - iteration 53, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:21:08] {1944} INFO -  at 1897.9s,\testimator extra_tree's best error=0.4984,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:21:08] {1748} INFO - iteration 54, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:21:10] {1944} INFO -  at 1900.5s,\testimator extra_tree's best error=0.4939,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:21:10] {1748} INFO - iteration 55, current learner xgboost\n",
      "[flaml.automl: 10-01 13:21:49] {1944} INFO -  at 1939.1s,\testimator xgboost's best error=0.4576,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:21:49] {1748} INFO - iteration 56, current learner rf\n",
      "[flaml.automl: 10-01 13:22:17] {1944} INFO -  at 1967.9s,\testimator rf's best error=0.5824,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:17] {1748} INFO - iteration 57, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:22:21] {1944} INFO -  at 1971.7s,\testimator extra_tree's best error=0.4939,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:21] {1748} INFO - iteration 58, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:22:23] {1944} INFO -  at 1973.9s,\testimator extra_tree's best error=0.4939,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:23] {1748} INFO - iteration 59, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:22:28] {1944} INFO -  at 1978.4s,\testimator extra_tree's best error=0.4728,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:28] {1748} INFO - iteration 60, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:22:31] {1944} INFO -  at 1981.0s,\testimator extra_tree's best error=0.4728,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:31] {1748} INFO - iteration 61, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:22:34] {1944} INFO -  at 1984.1s,\testimator extra_tree's best error=0.4728,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:34] {1748} INFO - iteration 62, current learner extra_tree\n",
      "[flaml.automl: 10-01 13:22:44] {1944} INFO -  at 1994.7s,\testimator extra_tree's best error=0.4526,\tbest estimator lgbm's best error=0.4429\n",
      "[flaml.automl: 10-01 13:22:44] {1748} INFO - iteration 63, current learner rf\n",
      "[flaml.automl: 10-01 13:22:50] {1944} INFO -  at 2000.9s,\testimator rf's best error=0.5824,\tbest estimator lgbm's best error=0.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 10-01 13:22:50] {2043} INFO - selected model: LGBMClassifier(learning_rate=0.037762750773915096, max_bin=128,\n",
      "               min_child_samples=18, n_estimators=351, num_leaves=597,\n",
      "               objective='multiclass', reg_alpha=0.0009765625,\n",
      "               reg_lambda=6.600587195294809, verbose=-1)\n",
      "[flaml.automl: 10-01 13:23:50] {2106} INFO - retrain lgbm for 59.9s\n",
      "[flaml.automl: 10-01 13:23:50] {2110} INFO - retrained model: LGBMClassifier(learning_rate=0.037762750773915096, max_bin=128,\n",
      "               min_child_samples=18, n_estimators=351, num_leaves=597,\n",
      "               objective='multiclass', reg_alpha=0.0009765625,\n",
      "               reg_lambda=6.600587195294809, verbose=-1)\n",
      "[flaml.automl: 10-01 13:23:50] {1539} INFO - fit succeeded\n",
      "[flaml.automl: 10-01 13:23:50] {1541} INFO - Time taken to find the best model: 849.0375683307648\n"
     ]
    }
   ],
   "source": [
    "automl2 = AutoML()\n",
    "automl2.fit(X_train, y_train,**settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ML leaner: lgbm\n",
      "Best hyperparmeter config: {'n_estimators': 351, 'num_leaves': 597, 'min_child_samples': 18, 'learning_rate': 0.037762750773915096, 'log_max_bin': 8, 'colsample_bytree': 1.0, 'reg_alpha': 0.0009765625, 'reg_lambda': 6.600587195294809}\n",
      "Best accuracy on validation data: 0.5571\n",
      "Training duration of best run: 331.9 s\n"
     ]
    }
   ],
   "source": [
    "''' retrieve best config and best learner'''\n",
    "print('Best ML leaner:', automl2.best_estimator)\n",
    "print('Best hyperparmeter config:', automl2.best_config)\n",
    "print('Best accuracy on validation data: {0:.4g}'.format(1-automl2.best_loss))\n",
    "print('Training duration of best run: {0:.4g} s'.format(automl2.best_config_train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn import neural_network\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### Neural Network (Solver=sgd)\n",
    "#######################\n",
    "\n",
    "estimator_MLP_B = MLPClassifier(batch_size='auto', warm_start=True, solver='sgd', max_iter=400, early_stopping=True)\n",
    "parameters_MLP_B = {\n",
    "    'hidden_layer_sizes': (64,128,64),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'learning_rate': ('constant', 'invscaling', 'adaptive'),\n",
    "    'momentum': (0.1,0.9,0.1),\n",
    "                   }\n",
    "# with GridSearch\n",
    "grid_search_MLP_B = GridSearchCV(\n",
    "    estimator=estimator_MLP_B,\n",
    "    param_grid=parameters_MLP_B,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = -1,\n",
    "    cv = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.conda\\envs\\mlhub\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MLP_1_B=grid_search_MLP_B.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_MLP1_B =MLP_1_B.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score - Neural Net (sgd) - Poly = 1: 0.5053152641402255\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Score - Neural Net (sgd) - Poly = 1:', metrics.accuracy_score(y_test, y_pred_MLP1_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
